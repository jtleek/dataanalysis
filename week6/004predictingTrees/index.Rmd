---
title       : Predicting with trees
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow  # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Key ideas

* Iteratively split variables into groups
* Split where maximally predictive
* Evaluate "homogeneity" within each branch
* Fitting multiple trees often works better (forests)

__Pros__:
* Easy to implement
* Easy to interpret
* Better performance in nonlinear settings

__Cons__:
* Without pruning/cross-validation can lead to overfitting
* Harder to estimate uncertainty
* Results may be variable


---

## Example Tree

<img class=center src=assets/img/obamaTree.png height='80%'/>

[http://graphics8.nytimes.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg](http://graphics8.nytimes.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg)

---

## Basic algorithm

1. Start with all variables in one group
2. Find the variable/split that best separates the outcomes
3. Divide the data into two groups ("leaves") on that split ("node")
4. Within each split, find the best variable/split that separates the outcomes
5. Continue until the groups are too small or sufficiently "pure"

---

## Measures of impurity

$$\hat{p}_{mk} = \frac{1}{N_m}\sum_{x_i\; in \; Leaf \; m}\mathbb{1}(y_i = k)$$

__Misclassification Error__: 
$$ 1 - \hat{p}_{mk(m)}$$

__Gini index__:
$$ \sum_{k \neq k'} \hat{p}_{mk} \times \hat{p}_{mk'} = \sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk}) $$

__Cross-entropy or deviance__:

$$ -\sum_{k=1}^K \hat{p}_{mk} \ln\hat{p}_{mk} $$


---

## Example: Iris Data
```{r iris}
data(iris)
names(iris)
table(iris$Species)
```
---

## Iris petal widths/sepal width
```{r ,dependson="iris",fig.height=4,fig.width=4}
plot(iris$Petal.Width,iris$Sepal.Width,pch=19,col=as.numeric(iris$Species))
legend(1,4.5,legend=unique(iris$Species),col=unique(as.numeric(iris$Species)),pch=19)
```


---

## Iris petal widths/sepal width
```{r createTree,dependson="iris"}
# An alternative is library(rpart)
library(tree)
tree1 <- tree(Species ~ Sepal.Width + Petal.Width,data=iris)
summary(tree1)
```

---

## Plot tree

```{r ,dependson="iris",fig.height=4.5,fig.width=4.5}
plot(tree1)
text(tree1)
```


---

## Another way of looking at a CART model

```{r ,dependson="createTree",fig.height=4.5,fig.width=4.5}
plot(iris$Petal.Width,iris$Sepal.Width,pch=19,col=as.numeric(iris$Species))
partition.tree(tree1,label="Species",add=TRUE)
legend(1.75,4.5,legend=unique(iris$Species),col=unique(as.numeric(iris$Species)),pch=19)
```

---

## Predicting new values

```{r newdata,dependson="createTree",fig.height=4.5,fig.width=4.5}
set.seed(32313)
newdata <- data.frame(Petal.Width = runif(20,0,2.5),Sepal.Width = runif(20,2,4.5))
pred1 <- predict(tree1,newdata)
pred1
```


---

## Overlaying new values

```{r ,dependson="newdata",fig.height=4.5,fig.width=4.5}
pred1 <- predict(tree1,newdata,type="class")
plot(newdata$Petal.Width,newdata$Sepal.Width,col=as.numeric(pred1),pch=19)
partition.tree(tree1,"Species",add=TRUE)
```

---

## Pruning trees example: Cars

```{r carsData,fig.height=3,fig.width=3}
data(Cars93,package="MASS")
head(Cars93)
```

---

## Build a tree

```{r, dependson="carsData",fig.height=4,fig.width=4}
treeCars <- tree(DriveTrain ~ MPG.city + MPG.highway + AirBags + 
                   EngineSize + Width + Length + Weight + Price + Cylinders + 
                   Horsepower + Wheelbase,data=Cars93)
plot(treeCars)
text(treeCars)
```

---

## Plot errors

```{r , dependson="treeCars",fig.height=4,fig.width=8}
par(mfrow=c(1,2))
plot(cv.tree(treeCars,FUN=prune.tree,method="misclass"))
plot(cv.tree(treeCars))
pruneTree <- prune.tree(treeCars,best=4)
plot(pruneTree)
text(pruneTree)
```

---

## Prune the tree

```{r pruneTree, dependson="treeCars",fig.height=4,fig.width=8,cache=TRUE}
pruneTree <- prune.tree(treeCars,best=4)
plot(pruneTree)
text(pruneTree)
```

---

## Show resubstitution error $^*$

```{r ,dependson="pruneTree",fig.height=4,fig.width=8}
table(Cars93$DriveTrain,predict(pruneTree,type="class"))
table(Cars93$DriveTrain,predict(treeCars,type="class"))
```

* Note that cross validation error is a better measure of test set accuracy


---

## Notes and further resources

* [Hector Corrada Bravo's Notes](http://www.cbcb.umd.edu/~hcorrada/PracticalML/pdf/lectures/trees.pdf), [code](http://www.cbcb.umd.edu/~hcorrada/PracticalML/src/trees.R)
* [Cosma Shalizi's notes](http://www.stat.cmu.edu/~cshalizi/350/lectures/22/lecture-22.pdf)
* [Elements of Statistical Learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
* [Classification and regression trees](http://www.amazon.com/Classification-Regression-Trees-Leo-Breiman/dp/0412048418)
* [Random forests](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)
